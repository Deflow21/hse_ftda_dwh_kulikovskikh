
services:
  postgres_master:
    container_name: postgres_master
    image: postgres:14.5
    restart: always
    volumes:
      - master-data:/var/lib/postgresql/data
      - ./init-script/master:/docker-entrypoint-initdb.d/
      - ./init-script/config/master/postgresql.conf:/etc/postgresql/postgresql.conf
      - ./init-script/config/master/pg_hba.conf:/etc/postgresql/pg_hba.conf
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: postgres
    command:
      - "postgres"
      - "-c"
      - "wal_level=logical"
      - "-c"
      - "max_replication_slots=5"
      - "-c"
      - "max_wal_senders=5"
      - "-c"
      - "config_file=/etc/postgresql/postgresql.conf"
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "postgres"]
      interval: 10s
      retries: 5
      start_period: 30s
      timeout: 10s
    networks:
      - default

  postgres_replica:
    container_name: postgres_replica
    image: postgres:14.5
    restart: always
    depends_on:
      - postgres_master
    volumes:
      - replica-data:/var/lib/postgresql/data
      - ./init-script/replica/entrypoint.sh:/docker-entrypoint-initdb.d/entrypoint.sh
      - ./init-script/config/replica/postgresql.conf:/etc/postgresql/postgresql.conf
      - ./init-script/config/replica/pg_hba.conf:/etc/postgresql/pg_hba.conf
    ports:
      - "5433:5432"
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      PGPASSWORD: replicator_password  # Используется в entrypoint.sh для подключения к master
    command:
      - "bash"
      - "/docker-entrypoint-initdb.d/entrypoint.sh"
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "postgres"]
      interval: 10s
      retries: 5
      start_period: 30s
      timeout: 10s
    networks:
      - default

  postgres_dwh:
    container_name: postgres_dwh
    image: postgres:14.5
    restart: always
    volumes:
      - dwh-data:/var/lib/postgresql/data
      - ./init-script/dwh:/docker-entrypoint-initdb.d/
      - ./init-script/config/dwh/postgresql.conf:/etc/postgresql/postgresql.conf
      - ./init-script/config/dwh/pg_hba.conf:/etc/postgresql/pg_hba.conf
    ports:
      - "5434:5432"
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres_dwh
      POSTGRES_DB: dwh_detailed
    command:
      - "postgres"
      - "-c"
      - "config_file=/etc/postgresql/postgresql.conf"
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "postgres"]
      interval: 10s
      retries: 5
      start_period: 30s
      timeout: 10s
    networks:
      - default

  zookeeper:
    container_name: zookeeper
    image: bitnami/zookeeper:latest
    restart: always
    environment:
      ALLOW_ANONYMOUS_LOGIN: "yes"  # ⚠️ Не используйте в продакшене
    networks:
      - default
    ports:
      - "2181:2181"

  kafka:
    container_name: kafka
    image: bitnami/kafka:latest
    restart: always
    depends_on:
      - zookeeper
    environment:
      ALLOW_PLAINTEXT_LISTENER: "yes"  # ⚠️ Не используйте в продакшене
      KAFKA_CFG_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_CFG_LISTENERS: PLAINTEXT://:9092
      KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR: "1"
      KAFKA_CFG_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE: "true"
    networks:
      - default
    ports:
      - "9092:9092"

  debezium:
    container_name: debezium
    image: debezium/connect:1.9
    platform: linux/amd64  # Оставьте, если необходимо
    restart: always
    depends_on:
      - kafka
      - postgres_master
    environment:
      BOOTSTRAP_SERVERS: kafka:9092
      GROUP_ID: "1"
      CONFIG_STORAGE_TOPIC: debezium_configs
      OFFSET_STORAGE_TOPIC: debezium_offsets
      STATUS_STORAGE_TOPIC: debezium_status
      CONFIG_STORAGE_REPLICATION_FACTOR: 1
      OFFSET_STORAGE_REPLICATION_FACTOR: 1
      STATUS_STORAGE_REPLICATION_FACTOR: 1
      KEY_CONVERTER_SCHEMAS_ENABLE: "false"
      VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_REST_HOST_NAME: 0.0.0.0
      CONNECT_REST_ADVERTISED_HOST_NAME: debezium
      CONNECT_REST_PORT: 8083
      LOG_LEVEL: DEBUG
      LOG4J_LOGGERS: "org.apache.kafka.connect.runtime.Worker=DEBUG,org.apache.kafka.connect.runtime.WorkerSinkTask=DEBUG"
      PLUGIN_PATH: /kafka/connect
    volumes:
      - ./connectors:/kafka/connect/connectors
      - ./init-script/config/connect-log4j.properties:/kafka/config/connect-log4j.properties
    networks:
      - default
    ports:
      - "8083:8083"

  init-debezium:
    image: appropriate/curl
    platform: linux/amd64
    container_name: init-debezium
    depends_on:
      - debezium
    volumes:
      - ./create_connector.sh:/create_connector.sh
      - ./connectors:/connectors
    entrypoint: ["sh", "/create_connector.sh"]
    networks:
      - default

  dmp_service:
    build: ./dmp-service
    container_name: dmp_service
    restart: always
    depends_on:
      - kafka
      - postgres_dwh
      - debezium
    environment:
      KAFKA_BROKER: kafka:9092
      DWH_HOST: postgres_dwh
      DWH_PORT: "5432"
      DWH_USER: postgres
      DWH_PASSWORD: postgres_dwh
      DWH_DB: dwh_detailed
    volumes:
      - ./dmp-service:/app
      - ./dmp-service/config.yaml:/app/config.yaml
    command: >
      sh -c "sleep 10 && python dmp_service.py"
    networks:
      - default

  init-kafka:
    image: bitnami/kafka:latest
    depends_on:
      - kafka
    entrypoint: [ '/bin/sh', '-c' ]
    command: |
      "
      # Ожидание, пока Kafka станет доступным
      until kafka-topics.sh --bootstrap-server kafka:9092 --list; do
        echo 'Waiting for Kafka to be ready...'
        sleep 5
      done;
      
      # Создание топиков
      echo 'Creating Kafka topics...'
      kafka-topics.sh --bootstrap-server kafka:9092 --create --if-not-exists --topic postgres_master.public.bookings --partitions 1 --replication-factor 1
      kafka-topics.sh --bootstrap-server kafka:9092 --create --if-not-exists --topic postgres_master.public.flights --partitions 1 --replication-factor 1
      kafka-topics.sh --bootstrap-server kafka:9092 --create --if-not-exists --topic postgres_master.public.tickets --partitions 1 --replication-factor 1
      echo 'Kafka topics created successfully.'
      
      kafka-topics.sh --bootstrap-server kafka:9092 --list"
    networks:
      - default

  postgres_airflow:
    container_name: postgres_airflow
    image: postgres:14.5
    restart: always
    volumes:
      - airflow-postgres-data:/var/lib/postgresql/data
      - ./init-script/config/airflow/postgresql.conf:/var/lib/postgresql/data/postgresql.conf
      - ./init-script/config/airflow/pg_hba.conf:/var/lib/postgresql/data/pg_hba.conf
    ports:
      - "5435:5432"
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow_password  # Установите надежный пароль
      POSTGRES_DB: airflow
      PGDATA: /var/lib/postgresql/data/db-files/
    command:
      - "postgres"
      - "-c"
      - "config_file=/var/lib/postgresql/data/db-files/postgresql.conf"
    healthcheck:
      test: [ "CMD", "pg_isready", "-U", "airflow" ]
      interval: 10s
      retries: 5
      start_period: 30s
      timeout: 10s
    networks:
      - default

  airflow-webserver:
    image: apache/airflow:2.6.1
    container_name: airflow-webserver
    restart: always
    depends_on:
      - postgres_airflow  # Добавлено
      - postgres_dwh
      - kafka
      - debezium
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow_password@postgres_airflow:5432/airflow
      AIRFLOW_CONN_POSTGRES_DWH: "postgresql://postgres:postgres_dwh@postgres_dwh:5432/dwh_detailed"
      AIRFLOW__CORE__FERNET_KEY: 'MEddFpD1s_DBq9l7BlzNGQJDNBEE8MlyWwcafoQr4Kk=' # Замените на ваш FERNET_KEY
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__WEBSERVER__SECRET_KEY: 'N9S57NcLEKOS1RIKZ9fdziiMdsu5tF5berLJeeaTf5s' # Замените на ваш SECRET_KEY
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    ports:
      - "8080:8080"
    command: webserver
    networks:
      - default

  airflow-scheduler:
    image: apache/airflow:2.6.1
    container_name: airflow-scheduler
    restart: always
    depends_on:
      - airflow-webserver
      - postgres_airflow  # Добавлено
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow_password@postgres_airflow:5432/airflow
      AIRFLOW_CONN_POSTGRES_DWH: "postgresql://postgres:postgres_dwh@postgres_dwh:5432/dwh_detailed"
      AIRFLOW__CORE__FERNET_KEY: 'MEddFpD1s_DBq9l7BlzNGQJDNBEE8MlyWwcafoQr4Kk=' # Замените на тот же FERNET_KEY
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    command: scheduler
    networks:
      - default

  airflow-init:
    image: apache/airflow:2.6.1
    container_name: airflow-init
    depends_on:
      postgres_airflow:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow_password@postgres_airflow:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: 'MEddFpD1s_DBq9l7BlzNGQJDNBEE8MlyWwcafoQr4Kk=' # Замените на тот же FERNET_KEY
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    entrypoint: >
      bash -c "
            airflow db init &&
            airflow connections add 'postgres_dwh' --conn-uri 'postgresql+psycopg2://postgres:postgres_dwh@postgres_dwh:5432/dwh_detailed' &&
            airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com &&
            exit
          "
    networks:
      - default

volumes:
  master-data:
  replica-data:
  dwh-data:
  airflow-dags:
  airflow-logs:
  airflow-plugins:
  airflow-postgres-data:

networks:
  default:
    driver: bridge